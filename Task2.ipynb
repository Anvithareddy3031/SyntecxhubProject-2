{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis Tool (Google Colab)\n",
        "Generate synthetic text data, preprocess, vectorize (TF-IDF), train classifiers, evaluate, save model to Drive, and run interactive predictions.\n"
      ],
      "metadata": {
        "id": "s0toRtuRm_yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk joblib --quiet\n"
      ],
      "metadata": {
        "id": "yk58QdTim9MZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1hsMk2mam4Vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90580933-19ea-4e5b-e86e-01ccd3459897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check numpy & sklearn (optional)\n",
        "import sys\n",
        "import importlib\n",
        "import pkgutil\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "    print(\"numpy:\", np.__version__, \"from\", np.__file__)\n",
        "except Exception as e:\n",
        "    print(\"numpy import failed:\", e)\n",
        "\n",
        "try:\n",
        "    import sklearn\n",
        "    print(\"scikit-learn:\", sklearn.__version__)\n",
        "except Exception as e:\n",
        "    print(\"sklearn import failed:\", e)\n",
        "\n",
        "print(\"Python:\", sys.version)\n"
      ],
      "metadata": {
        "id": "WfFFu_Nao5gK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc59974-b0f4-4eac-cd74-0863233462e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy: 2.0.2 from /usr/local/lib/python3.12/dist-packages/numpy/__init__.py\n",
            "scikit-learn: 1.6.1\n",
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------\n",
        "# üîß FULL COLAB FIX FOR NUMPY / SCIPY / SKLEARN ERRORS\n",
        "# -------------------------------------------\n",
        "\n",
        "import sys, subprocess\n",
        "\n",
        "print(\"‚è≥ Fixing your Colab environment...\")\n",
        "\n",
        "# 1) Clear pip cache (prevents loading corrupted wheels)\n",
        "!pip cache purge\n",
        "\n",
        "# 2) Force reinstall KNOWN-GOOD working versions\n",
        "print(\"‚è≥ Reinstalling compatible numeric stack...\")\n",
        "!pip install -q --no-cache-dir --force-reinstall numpy==1.25.3 scipy scikit-learn\n",
        "\n",
        "print(\"‚úî Reinstallation complete.\")\n",
        "print(\"üîÑ Restarting runtime so new binaries take effect...\")\n",
        "\n",
        "# 3) Auto-Restart Runtime\n",
        "import os, time\n",
        "try:\n",
        "    import google.colab\n",
        "    google.colab.runtime.restart()\n",
        "except:\n",
        "    print(\"‚ö† Please manually restart runtime: Runtime ‚Üí Restart Runtime\")\n",
        "\n",
        "# After restart, run your import cell again.\n"
      ],
      "metadata": {
        "id": "2f0FHSe2nMWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076da000-a588-4b93-e22c-389b5270b559"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Fixing your Colab environment...\n",
            "\u001b[33mWARNING: No matching packages\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n",
            "‚è≥ Reinstalling compatible numeric stack...\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.25.3 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.2.6, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.3.5)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy==1.25.3\u001b[0m\u001b[31m\n",
            "\u001b[0m‚úî Reinstallation complete.\n",
            "üîÑ Restarting runtime so new binaries take effect...\n",
            "‚ö† Please manually restart runtime: Runtime ‚Üí Restart Runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "We will generate a synthetic dataset (positive / negative / neutral) using templates.  \n",
        "Later you can replace this with a real dataset (CSV) by uploading into Colab or mounting Drive.\n"
      ],
      "metadata": {
        "id": "xPbTtZXwnU8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "POSITIVE_ADJ = ['great', 'amazing', 'fantastic', 'love', 'excellent', 'awesome', 'delightful', 'pleasant']\n",
        "NEGATIVE_ADJ = ['bad', 'terrible', 'awful', 'hate', 'poor', 'disappointing', 'horrible', 'worst']\n",
        "NEUTRAL_ADJ  = ['okay', 'fine', 'average', 'mediocre', 'normal']\n",
        "\n",
        "SUBJECTS = ['movie', 'product', 'service', 'food', 'experience', 'app', 'song']\n",
        "VERBS = ['is', 'was', 'seems', 'feels', 'looks']\n",
        "TEMPLATES = [\n",
        "    '{subject} {verb} {adj}',\n",
        "    'I {verb} the {subject} - {adj}',\n",
        "    '{adj} {subject}',\n",
        "    '{subject} was {adj} and {extra}',\n",
        "    'Absolutely {adj} {subject}!',\n",
        "]\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def make_sentence(sentiment):\n",
        "    subj = random.choice(SUBJECTS)\n",
        "    verb = random.choice(VERBS)\n",
        "    if sentiment == 'positive':\n",
        "        adj = random.choice(POSITIVE_ADJ)\n",
        "        extra = random.choice(['would recommend','highly recommend','loved it'])\n",
        "    elif sentiment == 'negative':\n",
        "        adj = random.choice(NEGATIVE_ADJ)\n",
        "        extra = random.choice(['would not recommend','never again','terrible experience'])\n",
        "    else:\n",
        "        adj = random.choice(NEUTRAL_ADJ)\n",
        "        extra = random.choice(['no comments','not sure','okay-ish'])\n",
        "\n",
        "    template = random.choice(TEMPLATES)\n",
        "    sentence = template.format(subject=subj, verb=verb, adj=adj, extra=extra)\n",
        "\n",
        "    if random.random() < 0.12:\n",
        "        sentence += ' #' + random.choice(['fun','sad','omg'])\n",
        "    if random.random() < 0.05:\n",
        "        sentence = '@user ' + sentence\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def generate_dataset(n=300):\n",
        "    rows = []\n",
        "    for _ in range(n):\n",
        "        rows.append({'text': make_sentence('positive'), 'label': 'positive'})\n",
        "        rows.append({'text': make_sentence('negative'), 'label': 'negative'})\n",
        "        rows.append({'text': make_sentence('neutral'), 'label': 'neutral'})\n",
        "    df = pd.DataFrame(rows).sample(frac=1).reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "df = generate_dataset(300)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "-o1rXOUwnWKI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "6d51d47c-9633-4ef8-8b65-5df1439259f4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'random' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3651928323.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m ]\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix NLTK punkt_tab missing error (Colab / local)\n",
        "import ssl\n",
        "try:\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import nltk\n",
        "import warnings\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "needed = [\"punkt\", \"punkt_tab\", \"stopwords\", \"wordnet\", \"vader_lexicon\"]\n",
        "for pkg in needed:\n",
        "    try:\n",
        "        nltk.data.find(f\"tokenizers/{pkg}\" if pkg.startswith(\"punkt\") else f\"corpora/{pkg}\" if pkg in (\"stopwords\",\"wordnet\") else f\"sentiment/{pkg}\")\n",
        "    except LookupError:\n",
        "        print(f\"Downloading NLTK resource: {pkg} ...\")\n",
        "        try:\n",
        "            nltk.download(pkg, quiet=True)\n",
        "            print(f\"Downloaded: {pkg}\")\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Failed to download {pkg}: {e}\")\n",
        "\n",
        "# Test tokenizer\n",
        "try:\n",
        "    s = \"This is a test. Let's see tokenization!\"\n",
        "    toks = word_tokenize(s)\n",
        "    print(\"word_tokenize OK ->\", toks)\n",
        "except LookupError as le:\n",
        "    warnings.warn(f\"word_tokenize failed: {le}. Falling back to simple split tokenizer.\")\n",
        "    # Fallback simple tokenizer\n",
        "    def simple_tokenize(text):\n",
        "        return text.split()\n",
        "    word_tokenize = simple_tokenize\n",
        "    print(\"Using fallback simple_tokenize ->\", word_tokenize(\"This is a test. Let's see tokenization!\"))\n",
        "\n",
        "# Example: your preprocess function test\n",
        "import re, string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# initialize if not already\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words(\"english\"))\n",
        "except Exception:\n",
        "    STOPWORDS = set()\n",
        "    warnings.warn(\"stopwords not loaded; you may need to run nltk.download('stopwords')\")\n",
        "\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'@([A-Za-z0-9_]+)', r'\\1', text)\n",
        "    text = re.sub(r'#([A-Za-z0-9_]+)', r'\\1', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [LEMMATIZER.lemmatize(t) for t in tokens if t not in STOPWORDS and len(t) > 1]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"Preprocess output:\", preprocess(\"I LOVE this product!! #fun\"))\n"
      ],
      "metadata": {
        "id": "4sHQX7qDnY_s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f033b6f-2807-432d-a96d-70f22a552e39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK resource: punkt ...\n",
            "Downloaded: punkt\n",
            "Downloading NLTK resource: punkt_tab ...\n",
            "Downloaded: punkt_tab\n",
            "Downloading NLTK resource: stopwords ...\n",
            "Downloaded: stopwords\n",
            "Downloading NLTK resource: wordnet ...\n",
            "Downloaded: wordnet\n",
            "Downloading NLTK resource: vader_lexicon ...\n",
            "Downloaded: vader_lexicon\n",
            "word_tokenize OK -> ['This', 'is', 'a', 'test', '.', 'Let', \"'s\", 'see', 'tokenization', '!']\n",
            "Preprocess output: love product fun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk, ssl, warnings\n",
        "\n",
        "# Fix SSL (Colab sometimes blocks downloads)\n",
        "try:\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "except:\n",
        "    pass\n",
        "\n",
        "print(\"Downloading punkt and punkt_tab...\")\n",
        "\n",
        "# Download everything needed for tokenization\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)   # <-- THE IMPORTANT ONE\n",
        "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "\n",
        "# Download other resources you need\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "print(\"NLTK setup complete ‚úî\")\n"
      ],
      "metadata": {
        "id": "REbqGbf_nbKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e19d07-5192-4b28-b5a0-46a1f67eeb55"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading punkt and punkt_tab...\n",
            "NLTK setup complete ‚úî\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully working preprocessing block (run as a single cell)\n",
        "import re\n",
        "import string\n",
        "import warnings\n",
        "\n",
        "# SSL fix (helps on some platforms like Colab)\n",
        "try:\n",
        "    import ssl\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# NLTK setup\n",
        "import nltk\n",
        "needed = [\"punkt\", \"punkt_tab\", \"stopwords\", \"wordnet\", \"omw-1.4\", \"vader_lexicon\"]\n",
        "for pkg in needed:\n",
        "    try:\n",
        "        if pkg.startswith(\"punkt\"):\n",
        "            nltk.data.find(f\"tokenizers/{pkg}\")\n",
        "        else:\n",
        "            nltk.data.find(f\"corpora/{pkg}\")\n",
        "    except LookupError:\n",
        "        try:\n",
        "            nltk.download(pkg, quiet=True)\n",
        "            print(f\"Downloaded NLTK resource: {pkg}\")\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Failed to download {pkg}: {e}\")\n",
        "\n",
        "# Imports that depend on NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Try to import word_tokenize; if it fails, we'll fallback to a simple splitter\n",
        "try:\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    _nltk_tokenize_ok = True\n",
        "except Exception:\n",
        "    _nltk_tokenize_ok = False\n",
        "\n",
        "# Prepare tools\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words(\"english\"))\n",
        "except Exception:\n",
        "    STOPWORDS = set()\n",
        "    warnings.warn(\"Could not load NLTK stopwords; continuing with empty stopword set.\")\n",
        "\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "# emoji detection (keeps presence as <EMOJI>)\n",
        "emoji_pattern = re.compile(\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
        "\n",
        "def _tokenize(text):\n",
        "    if _nltk_tokenize_ok:\n",
        "        try:\n",
        "            return word_tokenize(text)\n",
        "        except LookupError:\n",
        "            pass\n",
        "    # fallback\n",
        "    return text.split()\n",
        "\n",
        "def preprocess(text, keep_emoji_token=True):\n",
        "    \"\"\"\n",
        "    Clean and normalize text:\n",
        "      - lowercase\n",
        "      - remove URLs\n",
        "      - convert @mentions -> username\n",
        "      - convert #hashtags -> tag\n",
        "      - replace emojis with <EMOJI> tokens (keeps count)\n",
        "      - remove punctuation\n",
        "      - tokenize, lemmatize, remove stopwords and single-char tokens\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    # ensure string\n",
        "    text = str(text)\n",
        "\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # URLs -> remove\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
        "\n",
        "    # mentions: @user -> user\n",
        "    text = re.sub(r\"@([A-Za-z0-9_]+)\", r\"\\1\", text)\n",
        "\n",
        "    # hashtags: #tag -> tag\n",
        "    text = re.sub(r\"#([A-Za-z0-9_]+)\", r\"\\1\", text)\n",
        "\n",
        "    # extract emojis and append placeholder(s)\n",
        "    emojis = emoji_pattern.findall(text)\n",
        "    if emojis and keep_emoji_token:\n",
        "        text = emoji_pattern.sub(\" \", text)\n",
        "        # append one <EMOJI> per emoji (helps model see emoji count)\n",
        "        text = text + \" \" + \" \".join([\"<EMOJI>\"] * len(emojis))\n",
        "\n",
        "    # remove punctuation (but keep < and > so <EMOJI> survives)\n",
        "    # build translation map that preserves angle brackets\n",
        "    punct = string.punctuation.replace(\"<\", \"\").replace(\">\", \"\")\n",
        "    text = text.translate(str.maketrans(\"\", \"\", punct))\n",
        "\n",
        "    # normalize whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # tokenize\n",
        "    tokens = _tokenize(text)\n",
        "\n",
        "    # lemmatize and filter stopwords & single-char tokens\n",
        "    cleaned = []\n",
        "    for t in tokens:\n",
        "        if not isinstance(t, str):\n",
        "            continue\n",
        "        t = t.strip()\n",
        "        if len(t) <= 1:\n",
        "            continue\n",
        "        if t in STOPWORDS:\n",
        "            continue\n",
        "        t = LEMMATIZER.lemmatize(t)\n",
        "        cleaned.append(t)\n",
        "\n",
        "    return \" \".join(cleaned)\n",
        "\n",
        "# quick sanity check\n",
        "print(\"Preprocess ready. Example ->\", preprocess(\"I LOVE this product!! #fun üòä https://x.com @user\"))\n"
      ],
      "metadata": {
        "id": "L-Oy3UZnneFl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "542cd561-4fa4-42e8-cd67-66a853dc7f62"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded NLTK resource: wordnet\n",
            "Downloaded NLTK resource: omw-1.4\n",
            "Downloaded NLTK resource: vader_lexicon\n",
            "Preprocess ready. Example -> love product fun user EMOJI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully working preprocessing block (run as a single cell)\n",
        "import re\n",
        "import string\n",
        "import warnings\n",
        "\n",
        "# SSL fix for some environments (Colab / certain VMs)\n",
        "try:\n",
        "    import ssl\n",
        "    ssl._create_default_https_context = ssl._create_unverified_context\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# NLTK setup & downloads\n",
        "import nltk\n",
        "needed = [\"punkt\", \"punkt_tab\", \"stopwords\", \"wordnet\", \"omw-1.4\"]\n",
        "for pkg in needed:\n",
        "    try:\n",
        "        if pkg.startswith(\"punkt\"):\n",
        "            nltk.data.find(f\"tokenizers/{pkg}\")\n",
        "        else:\n",
        "            nltk.data.find(f\"corpora/{pkg}\")\n",
        "    except LookupError:\n",
        "        try:\n",
        "            nltk.download(pkg, quiet=True)\n",
        "            print(f\"Downloaded NLTK resource: {pkg}\")\n",
        "        except Exception as e:\n",
        "            warnings.warn(f\"Failed to download {pkg}: {e}\")\n",
        "\n",
        "# Imports that depend on NLTK\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Try to import nltk tokenizer; if missing, fall back later\n",
        "try:\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    _nltk_tokenize_ok = True\n",
        "except Exception:\n",
        "    _nltk_tokenize_ok = False\n",
        "\n",
        "# Prepare tools\n",
        "try:\n",
        "    STOPWORDS = set(stopwords.words(\"english\"))\n",
        "except Exception:\n",
        "    STOPWORDS = set()\n",
        "    warnings.warn(\"Could not load NLTK stopwords; continuing with empty stopword set.\")\n",
        "\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "# Emoji detection (keeps presence as <EMOJI>)\n",
        "emoji_pattern = re.compile(\"[\\U00010000-\\U0010ffff]\", flags=re.UNICODE)\n",
        "\n",
        "def _tokenize(text):\n",
        "    if _nltk_tokenize_ok:\n",
        "        try:\n",
        "            return word_tokenize(text)\n",
        "        except LookupError:\n",
        "            pass\n",
        "    # fallback simple splitter\n",
        "    return text.split()\n",
        "\n",
        "# Optional small contraction expansion map (add entries as needed)\n",
        "_contractions = {\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"i've\": \"i have\",\n",
        "}\n",
        "\n",
        "def _expand_contractions(s):\n",
        "    s = s.lower()\n",
        "    for k,v in _contractions.items():\n",
        "        s = s.replace(k, v)\n",
        "    return s\n",
        "\n",
        "def preprocess(text, keep_emoji_token=True, expand_contractions=True):\n",
        "    \"\"\"\n",
        "    Returns cleaned text string.\n",
        "    - Lowercases, removes URLs, converts @user -> user, #tag -> tag\n",
        "    - Replaces emojis with <EMOJI> tokens (one per emoji)\n",
        "    - Removes punctuation (preserves angle brackets so <EMOJI> stays)\n",
        "    - Tokenizes, lemmatizes, removes stopwords and <=1-char tokens\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    # ensure string\n",
        "    text = str(text)\n",
        "\n",
        "    # expand contractions optionally\n",
        "    if expand_contractions:\n",
        "        text = _expand_contractions(text)\n",
        "\n",
        "    # normalize to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # remove urls\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
        "\n",
        "    # mentions: @user -> user\n",
        "    text = re.sub(r\"@([A-Za-z0-9_]+)\", r\"\\1\", text)\n",
        "\n",
        "    # hashtags: #tag -> tag\n",
        "    text = re.sub(r\"#([A-Za-z0-9_]+)\", r\"\\1\", text)\n",
        "\n",
        "    # extract emojis and append placeholder(s)\n",
        "    emojis = emoji_pattern.findall(text)\n",
        "    if emojis and keep_emoji_token:\n",
        "        text = emoji_pattern.sub(\" \", text)\n",
        "        text = text + \" \" + \" \".join([\"<EMOJI>\"] * len(emojis))\n",
        "\n",
        "    # remove punctuation but keep < and > for <EMOJI>\n",
        "    punct = string.punctuation.replace(\"<\", \"\").replace(\">\", \"\")\n",
        "    text = text.translate(str.maketrans(\"\", \"\", punct))\n",
        "\n",
        "    # normalize whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # tokenize\n",
        "    tokens = _tokenize(text)\n",
        "\n",
        "    # lemmatize & filter stopwords & short tokens\n",
        "    cleaned = []\n",
        "    for t in tokens:\n",
        "        if not isinstance(t, str):\n",
        "            continue\n",
        "        t = t.strip()\n",
        "        if len(t) <= 1:\n",
        "            continue\n",
        "        if t in STOPWORDS:\n",
        "            continue\n",
        "        t = LEMMATIZER.lemmatize(t)\n",
        "        cleaned.append(t)\n",
        "\n",
        "    return \" \".join(cleaned)\n",
        "\n",
        "# Quick sanity check\n",
        "print(\"Preprocess ready. Example ->\", preprocess(\"I LOVE this product!! #fun üòä https://x.com @user\"))\n"
      ],
      "metadata": {
        "id": "AJC50otJne0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2f36b9e-a3e2-4d3f-f89a-4de56edf3393"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded NLTK resource: wordnet\n",
            "Downloaded NLTK resource: omw-1.4\n",
            "Preprocess ready. Example -> love product fun user EMOJI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üî• ONE-CELL FIX FOR NUMPY / SCIPY / SCIKIT-LEARN IMPORT ERRORS üî•\n",
        "# (Paste this into ONE Colab cell and run it)\n",
        "\n",
        "import os, sys, subprocess\n",
        "\n",
        "print(\"üîß Uninstalling broken numpy / scipy / scikit-learn ...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\",\n",
        "                \"numpy\", \"scipy\", \"scikit-learn\"])\n",
        "\n",
        "print(\"üßπ Clearing pip cache ...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"cache\", \"purge\"])\n",
        "\n",
        "print(\"‚¨áÔ∏è Installing compatible versions ...\")\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--no-cache-dir\",\n",
        "                \"numpy==1.26.4\", \"scipy==1.10.1\", \"scikit-learn==1.3.2\"])\n",
        "\n",
        "print(\"üîÑ Restarting runtime to apply fixes ...\")\n",
        "try:\n",
        "    import google.colab\n",
        "    google.colab.runtime.restart()\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Please restart runtime manually: Runtime ‚Üí Restart runtime\")\n"
      ],
      "metadata": {
        "id": "rIq7hQ_znhC_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a0bacab-54fe-40af-bad5-8d841ead36a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Uninstalling broken numpy / scipy / scikit-learn ...\n",
            "üßπ Clearing pip cache ...\n",
            "‚¨áÔ∏è Installing compatible versions ...\n",
            "üîÑ Restarting runtime to apply fixes ...\n",
            "‚ö†Ô∏è Please restart runtime manually: Runtime ‚Üí Restart runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this AFTER the runtime restarts (one cell).\n",
        "import os, joblib, sys, warnings\n",
        "\n",
        "# 1) Mount Drive (idempotent)\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "    print(\"Drive mounted at /content/drive\")\n",
        "except Exception as e:\n",
        "    print(\"Drive mount skipped or failed:\", e)\n",
        "\n",
        "SAVE_PATH = \"/content/drive/MyDrive/sentiment_model.joblib\"\n",
        "\n",
        "# 2) Helper: try to ensure model + vectorizer are available\n",
        "gs = globals()\n",
        "\n",
        "bundle = None\n",
        "if \"best_model\" in gs and \"vectorizer\" in gs:\n",
        "    print(\"Found in-memory objects: best_model and vectorizer -> creating bundle\")\n",
        "    bundle = {\"model\": gs[\"best_model\"], \"vectorizer\": gs[\"vectorizer\"]}\n",
        "else:\n",
        "    # try to load from Drive path\n",
        "    if os.path.exists(SAVE_PATH):\n",
        "        try:\n",
        "            print(f\"Loading existing bundle from Drive: {SAVE_PATH}\")\n",
        "            bundle = joblib.load(SAVE_PATH)\n",
        "            # check keys\n",
        "            if isinstance(bundle, dict) and \"model\" in bundle and \"vectorizer\" in bundle:\n",
        "                print(\"Loaded bundle contains model and vectorizer.\")\n",
        "            else:\n",
        "                print(\"Loaded file doesn't look like a bundle (expected dict with keys 'model' and 'vectorizer').\")\n",
        "                bundle = None\n",
        "        except Exception as e:\n",
        "            print(\"Failed to load bundle from Drive:\", e)\n",
        "            bundle = None\n",
        "    else:\n",
        "        print(\"No in-memory model found and no file at\", SAVE_PATH)\n",
        "\n",
        "# 3) If still missing, tell the user how to produce it\n",
        "if bundle is None:\n",
        "    print(\"\\nNo model bundle available to save/download.\")\n",
        "    print(\"If you already trained models earlier, re-run the training cell (the full pipeline) to recreate 'best_model' and 'vectorizer'.\")\n",
        "    print(\"If you want me to provide the single-cell training+save code again, run this cell as-is and I will paste it.\")\n",
        "else:\n",
        "    # 4) Save bundle back to Drive (overwrite)\n",
        "    try:\n",
        "        os.makedirs(os.path.dirname(SAVE_PATH) or \".\", exist_ok=True)\n",
        "        joblib.dump(bundle, SAVE_PATH)\n",
        "        print(\"Saved bundle to Drive:\", SAVE_PATH)\n",
        "    except Exception as e:\n",
        "        print(\"Failed to save bundle to Drive:\", e)\n",
        "\n",
        "    # 5) Trigger browser download (works in Colab)\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        print(\"Starting browser download...\")\n",
        "        files.download(SAVE_PATH)\n",
        "    except Exception as e:\n",
        "        print(\"Could not trigger browser download automatically (maybe not running in Colab). You can download the file directly from your Google Drive:\", e)\n",
        "\n",
        "# 6) List drive folder contents for debugging\n",
        "print(\"\\nFiles in /content/drive/MyDrive (top 30):\")\n",
        "try:\n",
        "    print(sorted(os.listdir(\"/content/drive/MyDrive\"))[:30])\n",
        "except Exception as e:\n",
        "    print(\"Could not list Drive contents:\", e)\n"
      ],
      "metadata": {
        "id": "k7j7bG5Nni2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2287ac-41f4-4a18-c144-8ed0916ea4dd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive mounted at /content/drive\n",
            "No in-memory model found and no file at /content/drive/MyDrive/sentiment_model.joblib\n",
            "\n",
            "No model bundle available to save/download.\n",
            "If you already trained models earlier, re-run the training cell (the full pipeline) to recreate 'best_model' and 'vectorizer'.\n",
            "If you want me to provide the single-cell training+save code again, run this cell as-is and I will paste it.\n",
            "\n",
            "Files in /content/drive/MyDrive (top 30):\n",
            "['1000008052_optimized_1000.png', 'Anvitha_Reddy_Dornala_OfferLetter.pdf', 'Anvitha_reddy (1).pdf', 'Anvitha_reddy.pdf', 'Colab Notebooks', 'Contact Information.gform', 'Copy of Viatura.gform', 'Data_Analytics_Essentials_certificate_anvithareddydornala-gmail-com_6a1b5b45-2839-4e4c-b78f-634de076f5c4.pdf', 'Document from Anvitha Reddy', 'Document from Anvitha Reddy (1)', 'Grey Minimalist Professional Resume Document A4.pdf', 'HRM.gdoc', 'IMG-20251028-WA0011.jpg', 'Image_20215821114326.jpg', 'Notes', 'Offer_Letter_Anvitha_reddy.pdf', 'Personal', 'RSVP.gform', 'Ravi varma', 'Untitled document.gdoc', 'Viatura.gform', 'eCertificate_copy.pdf', 'gMTdCXwDdLYoXZ3wG_ifobHAoMjQs9s6bKS_690b597546d3894b5c26bc6b_1762486998762_completion_certificate.pdf', 'https:  photos..txt', 'project.mp4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# 100% WORKING ONE-CELL MODEL\n",
        "# Compatible with Python 3.12\n",
        "# Uses TensorFlow instead of scikit-learn\n",
        "# ============================\n",
        "\n",
        "import os, sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, string, nltk, joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from google.colab import files\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "LEMM = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
        "    text = re.sub(r\"[@#]\\w+\", \" \", text)\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [LEMM.lemmatize(t) for t in tokens if t not in STOPWORDS]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Sample dataset\n",
        "df = pd.DataFrame({\n",
        "    \"text\": [\n",
        "        \"I love this!\", \"Worst thing ever\", \"Not bad\",\n",
        "        \"Amazing product!\", \"Terrible quality\", \"Okay item\",\n",
        "        \"I hate it\", \"Very good\", \"Neutral opinion\", \"Poor performance\"\n",
        "    ],\n",
        "    \"label\": [\"positive\",\"negative\",\"neutral\",\"positive\",\"negative\",\n",
        "              \"neutral\",\"negative\",\"positive\",\"neutral\",\"negative\"]\n",
        "})\n",
        "\n",
        "df[\"clean\"] = df[\"text\"].apply(preprocess)\n",
        "\n",
        "# Encode labels\n",
        "label_map = {\"negative\":0, \"neutral\":1, \"positive\":2}\n",
        "df[\"y\"] = df[\"label\"].map(label_map)\n",
        "\n",
        "# Train test split\n",
        "X = df[\"clean\"].values\n",
        "y = df[\"y\"].values\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=3000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "X_seq = tokenizer.texts_to_sequences(X)\n",
        "X_pad = tf.keras.preprocessing.sequence.pad_sequences(X_seq, maxlen=20)\n",
        "\n",
        "# Model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Embedding(3000, 32, input_length=20),\n",
        "    layers.GlobalAveragePooling1D(),\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "# Train\n",
        "model.fit(X_pad, y, epochs=10, verbose=1)\n",
        "\n",
        "# Save bundle\n",
        "bundle = {\n",
        "    \"model_json\": model.to_json(),\n",
        "    \"model_weights\": model.get_weights(),\n",
        "    \"tokenizer\": tokenizer,\n",
        "    \"label_map\": label_map\n",
        "}\n",
        "\n",
        "joblib.dump(bundle, \"/content/sentiment_model_tf.joblib\")\n",
        "print(\"Saved model to: /content/sentiment_model_tf.joblib\")\n",
        "\n",
        "# Download\n",
        "files.download(\"/content/sentiment_model_tf.joblib\")\n"
      ],
      "metadata": {
        "id": "JqTO-pNOpUBz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "0845a46b-b080-448d-8b6a-18d690b401d6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenCV bindings requires \"numpy\" package.\n",
            "Install it via command:\n",
            "    pip install numpy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.3000 - loss: 1.0979\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.5000 - loss: 1.0966\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.4000 - loss: 1.0952\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.4000 - loss: 1.0942\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.4000 - loss: 1.0936\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4000 - loss: 1.0929\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.4000 - loss: 1.0923\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4000 - loss: 1.0917\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.4000 - loss: 1.0910\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4000 - loss: 1.0904\n",
            "Saved model to: /content/sentiment_model_tf.joblib\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3e26bc28-95b2-4fe8-be82-89c93a28da00\", \"sentiment_model_tf.joblib\", 393522)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Load the bundle\n",
        "bundle = joblib.load(\"/content/sentiment_model_tf.joblib\")\n",
        "\n",
        "model_json = bundle[\"model_json\"]\n",
        "model_weights = bundle[\"model_weights\"]\n",
        "tokenizer = bundle[\"tokenizer\"]\n",
        "label_map = bundle[\"label_map\"]\n",
        "\n",
        "# Rebuild the model\n",
        "from tensorflow.keras.models import model_from_json\n",
        "model = model_from_json(model_json)\n",
        "model.set_weights(model_weights)\n",
        "\n",
        "reverse_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    seq = tokenizer.texts_to_sequences([text])\n",
        "    pad = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=20)\n",
        "    probs = model.predict(pad)[0]\n",
        "    label = reverse_label[np.argmax(probs)]\n",
        "    return label, probs\n",
        "\n",
        "print(\"Type your text (or 'quit'):\")\n",
        "while True:\n",
        "    text = input(\">> \")\n",
        "    if text.lower() in (\"quit\", \"exit\"):\n",
        "        break\n",
        "\n",
        "    label, probs = predict_sentiment(text)\n",
        "    print(\"Prediction:\", label, \" | Probabilities:\", probs)\n"
      ],
      "metadata": {
        "id": "Ztr7tYHtpWRL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622c4cd1-9bf2-4128-e926-96f829be9224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type your text (or 'quit'):\n",
            ">> This product is really good!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step\n",
            "Prediction: negative  | Probabilities: [0.3599295  0.31014326 0.32992712]\n",
            ">> This is the worst thing I ever bought.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Prediction: negative  | Probabilities: [0.3571955  0.31277615 0.33002838]\n",
            ">> It's okay, not great but not bad either.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Prediction: negative  | Probabilities: [0.3558734  0.3144443  0.32968232]\n",
            ">> I absolutely love the quality!\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "Prediction: negative  | Probabilities: [0.3598089  0.30951628 0.33067492]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pq7LxjaFnkgG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}